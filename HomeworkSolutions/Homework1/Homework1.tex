\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,%
            left=.75in,right=.75in,top=1in,bottom=1in]{geometry}
\setlength{\headsep}{0.25in}

\usepackage{amsthm}

\usepackage{graphicx}
\usepackage{pgfplots}

\usepackage{hyperref}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
            
\usepackage[english]{babel}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{case}{Case}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\usepackage{mathtools}
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}

\DeclareMathOperator{\interior}{int}

\newcommand{\Tau}{\mathcal{T}}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\usepackage{calligra}
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}
\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[2.2]callig15}{}

\newcommand{\scripty}[1]{\ensuremath{\mathcalligra{#1}}}

\pagestyle{fancy}
\author{Jeremiah Givens}
\newcommand{\subject}{Probability MA 585}
\newcommand{\Date}{9/2/2021} 
\makeatletter
\rhead{{\small\@author}}
\lhead{{\small\subject}}
\chead{{\large Homework 1}}
\cfoot{}
\rfoot{\thepage}
\lfoot{\today}

\renewcommand{\theequation}{\arabic{equation}}

\begin{document}
\section*{Problem 1}
Suppose one is rolling a six-sided die. Let $\mathcal{F}$ the smallest $\sigma$-field containing $A = \{5, 6\}$
and $B = \{2, 4, 6\}$.  Prove that $\{5\}$,  $\{1, 3\}$ and $\{1, 2, 3, 4\}$ are events for the measurable space
$(\Omega,\mathcal{F})$.

\subsection*{Solution}

\begin{proof}
Since $\mathcal{F}$ is a $\sigma$-algebra, we have that $\mathcal{F}$ is closed under complements and countable unions (and countable intersections, by D'Morgan's Law). In addition, $\mathcal{F}$ contains $\Omega = \{1, ...,6\}$, and the empty set $\emptyset$. Thus, if we can show that each of the three sets in the problem can be written as a finite union and/or intersection of $A, B$ and their complements, then our proof will be complete.

We have 
\begin{align*}
B^c \cap (A \cup B) &= \{1, 3, 5\} \cap \{2, 4, 5, 6\}\\
&= \{5\},
\end{align*}
and
\begin{align*}
B^c \cap A^c &= \{1, 3, 5\} \cap \{1, ..., 4\}\\
\{1, 3\},
\end{align*}
and
\begin{align*}
A^c = \{1, 2, 3, 4\}.
\end{align*}
With this, our proof is complete.
\end{proof}

\section*{Problem 2}
Let $A$ and $B$ be a pair of independent events. Prove that $A^c$ and $B^c$ are independent.

\subsection*{Solution}
\begin{proof}
We can see
\begin{align*}
P(A^c \cap B^c) &= P((A \cup B)^c) && \text{D'Morgan's Law}\\
&= 1 - P(A \cup B)\\
&= 1 - (P(A) + P(B) - P(A \cap B)) && \text{We derived this in class}\\
&= 1 - (P(A) + P(B) - P(A)P(B)) && \text{By independence of } A,B\\
&= 1 - (P(A) + P(B)(1-P(A))\\
&= 1 - (P(A) + P(B)P(A^c))\\
&= 1 - P(A) - P(B)P(A^c)\\
&= P(A^c) - P(B)P(A^c)\\
&= P(A^c)(1  - P(B))\\
&= P(A^c)P(B^c),
\end{align*}
and we can conclude that $A^c$ and $B^c$ are independent.
\end{proof}

\section*{Problem 3}
Consider a coin-die experiment: One flips a fair coin at first. If he gets a head, then he will
roll a 6-sided fair die; otherwise, he will roll a 4-sided unfair die, which has probability $\frac{5 - i}
{10}$ to
get the $i^\text{th}$ face up, where $i \in \{1,...,4\}$.  If one gets a 2 face up, what is the probability that they got a tail
when they flipped the coin?

\subsection*{Solution}
Let us denote the result of the coin flip as $X$.  Then, either $X = H$ or $X = T$ for a given experiment. Let us denote the result of the die roll by the random variable $Y$. Let $A = \{X = T\}$ and $B = \{Y = 2\}$ be two events. By definition of conditional probability, we have
\begin{align*}
P(A|B) = \frac{P(A \cup B)}{P(B)} \text{, and } P(B|A) = \frac{P(A \cup B)}{P(A)} \implies P(A|B) = \frac{P(B|A) P(A)}{P(B)}.
\end{align*}
Now, since $\{X =H\}$ and $\{X = T\}$ partitions our sample space, we have that 
\begin{align*}
P(B) = P(B \cap \{X = H\}) + P(B \cap \{X = T\}) =  P(B| X = H) P(X = H) + P(B| X = T) P(X = T).
\end{align*}

Thus, the equation of interest becomes
\begin{align}
P(A|B) = \frac{P(B|A) P(A)}{P(B| X = H) P(X = H) + P(B| X = T) P(X = T)}.
\end{align}
The probabilities involved are all essentially given in the problem statement:
\begin{align*}
&P(A) = \frac{1}{2} = P(X = T) = P(X = H) && \text{because we flip a fair coin}\\
&P(B|X = H) = P(Y = 2| X = H) = \frac{1}{6} && \text{because we roll a fair die}\\
&P(B|A) = P(B|X = T) = P(Y = 2| X = T) = \frac{5 - 2}{10} = \frac{3}{10} && \text{because flip our weighted die}
\end{align*}
Plugging these into (1), we have
\begin{align*}
P(A|B) &= \frac{\frac{3}{10} \frac{1}{2}}{\frac{1}{6}  \frac{1}{2} + \frac{3}{10} \frac{1}{2}}\\
&= \frac{\frac{3}{20}}{\frac{1}{12} + \frac{3}{20}}\\
&= \frac{3}{\frac{5 + 9}{3}}\\
&= \frac{9}{14}.
\end{align*}
For a sanity check, the code for our numerical simulation can be found \href{https://github.com/jeremiahgivens/MA-585-Probability/blob/main/HomeworkSolutions/Homework1/Homework1Problem3.py}{at my github}, which supports our conclusion.

\section*{Problem 4}
A series of Bernoulli trials with successful rate $p \in (0, 1)$ is performed. We will stop the
experiment whenever a changeover occurs, which means that the outcome differs from the
one preceding it. Let $X$ denote the number of Bernoulli trials being performed. Prove that
$P(X \geq 3) \geq 0.5$.

\subsection*{Solution}
\begin{proof}
Let $Y_n \in \{1, 2\}$ be the outcome of the $n^{\text{th}}$ trial in our experiment, where $P(Y_n = 1) = p$, and $P(Y_n = 2) = 1 - p$.  We have
\begin{align*}
P(X \geq 3) &= 1 - P(X < 3)\\
&= 1 - P(X = 2) && \text{since } P(X = 1) = 0
\end{align*}

Now we have a partition of formed by the events $Y_1 = 1$, and $Y_1 = 2$. Thus, we have
\begin{align*}
P(X = 2) &= P(X = 2 \land Y_1 = 1) + P(X = 2 \land Y_1 = 2)\\
&= P(Y_1 = 1) P(X = 2| Y_1 = 1) + P(Y_1 = 2) P(X = 2| Y_1 = 2) && \text{from definition of conditional probability}\\
&= p(1-p) + (1 - p) p\\
&= 2p(1 - p)\\
&= 2p - 2p^2.
\end{align*}
Thus,
\begin{align*}
P(X \geq 3) & =  1 - P(X = 2)\\
&= 2p^2 - 2p + 1.
\end{align*}
The aim of this problem is to show that $P(X = 2)$ has a lower bound of $0.5$ for all allowed values of $p$. Thus, all we need to do is show that $P(X = 2) \geq 0.5$ at its minimum. We have
\begin{align*}
\frac{d}{d p}(P(X \geq 3)) &= \frac{d}{d p}(2p^2 - 2p + 1)\\
&= 4p - 2,
\end{align*}
and
\begin{align*}
\frac{d^2}{d p^2}(P(X \geq 3)) &= 4.
\end{align*}
Our second derivative is positive for all values of $p$, which means that any critical point gives us a local minimum. Setting our first derivative equal to $0$, we see
\begin{align*}
p = \frac{1}{2}.
\end{align*}
Plugging this into our expression for $P(X \geq 3)$, 
\begin{align*}
P(X \geq 3) &= 2(\frac{1}{2})^2 - 2 \frac{1}{2} + 1\\
&= \frac{1}{2} - 1 + 1\\
&= \frac{1}{2}.
\end{align*}
Since this is a local minimum, we have proven that $P(X \geq 3) \geq 0.5$.
\end{proof}

\section*{Problem 5 (Bonus Problem)}
Suppose a drunk secretary prepares n letters
and corresponding envelopes to send to $n$ different people,  but then stuffs the letters in the
envelopes randomly. (1). Find the probability $p_n$ that at least one letter is inserted into the
proper envelope. (2) Find $\lim_{n \to \infty} p_n$.

\subsection*{Solution}
Let us first consider all of the ways in which the secretary could distribute the letters. In the first envelope, she has $n$ choices.  In the second envelope, she has $n - 1$ letters to choose from. Continuing in this manner, we can see that there are $n!$ ways for the secretary to distribute the letters. Now all that remains is to find how many ways she can distribute the letters that will lead to at least one of them going to the proper place.

Denote $A_k$ as the set of all ways that the secretary could distribute the letters in which the $k^{\text{th}}$ letter goes to the proper place, for $k \in \{1, ..., n\}$. Then,  the union 
\begin{align*}
\bigcup_{k=1}^n A_k
\end{align*}
is the set of all distributions of letters and envelopes that leads to at least one letter going to the correct location. Thus, our problem has boiled down to finding the cardinality of this set. From the inclusion-exclusion principle, we have
\begin{align*}
|\bigcup_{k=1}^n A_k| &= \sum_{k=1}^n (-1 )^{k+1} \left( \sum_{1 \leq i_1 \leq ... \leq i_k \leq n} |A_{i_1} \cap ... \cap A_{i_k}| \right).
\end{align*}
Now,  each term in the right sum corresponds to fixing $k$ letters in the correct envelopes, and mixing up all of the others. Thus,  for each term, there are ${n \choose k}$ combinations of correct letters, and $(n-k)!$ permutations for the incorrect letters. Therefore, this becomes
\begin{align*}
|\bigcup_{k=1}^n A_k| &= \sum_{k=1}^n (-1 )^{k+1} {n \choose k} (n-k)!\\
&= \sum_{k=1}^n (-1 )^{k+1} \frac{n!}{k! (n-k)!}(n-k)!\\
&= \sum_{k=1}^n (-1 )^{k+1} \frac{n!}{k!}\\
&= n! \sum_{k=1}^n  \frac{(-1 )^{k+1}}{k!}.
\end{align*}
Dividing by the total numbers of ways the secretary can send the letters, we have
\begin{align*}
P(\text{At least one person recieves correct letter}) &= \frac{|\bigcup_{k=1}^n A_k| }{n!} \\
&= \sum_{k=1}^n  \frac{(-1 )^{k+1}}{k!}.
\end{align*}
Taking the limit as $n$ goes to infinity, we have
\begin{align*}
P(\text{At least one person recieves correct letter}) &= \sum_{k=1}^\infty  \frac{(-1 )^{k+1}}{k!}.
\end{align*}
Recalling our Taylor series, we can see this fits the coefficients for the Taylor series, centered about 0, of $1 - e^{-x}$:
\begin{align*}
1 - e^{-x} = \sum_{k=0}^n  \frac{(-1 )^{k+1}}{k!} x.
\end{align*}
Notice the zeroth term is $0$.  Evaluating this at $x = 1$, we have 
\begin{align*}
P(\text{At least one person recieves correct letter}) &= 1 - e^{-1}.
\end{align*}
Once more, given the challenge of the problem, I've written a numerical simulation in Python that supports our conclusion.  You can find the code for this \href{https://github.com/jeremiahgivens/MA-585-Probability/blob/main/HomeworkSolutions/Homework1/Homework1BonusProblem.py}{here}.
\end{document}